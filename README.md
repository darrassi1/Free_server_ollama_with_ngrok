# Ollama on Google Colab with Ngrok

## Introduction

Running Large Language Models (LLMs) on personal devices often leads to slow performance and overheating. Fortunately, there's a free solution: using Google Colab, Ollama, and Ngrok for efficient LLM inference.


### Meet Google Colab

Google Colab offers free access to computing resources, including GPUs, making it ideal for running LLMs without extra costs.

## Setting Up Free LLM Inference

### Prerequisites

1. **Google Account**: Create a new notebook on Google Colab.
2. **Ngrok Authtoken**: Create an Ngrok account, navigate to Tunnels > Authtokens, and add a token.
# Ollama on Google Colab with Ngrok

## Introduction

Running Large Language Models (LLMs) on personal devices often leads to slow performance and overheating. Fortunately, there's a free solution: using Google Colab, Ollama, and Ngrok for efficient LLM inference.


### Meet Google Colab

Google Colab offers free access to computing resources, including GPUs, making it ideal for running LLMs without extra costs.

## Setting Up Free LLM Inference

### Prerequisites

1. **Google Account**: Create a new notebook on Google Colab.
2. **Ngrok Authtoken**: Create an Ngrok account, navigate to Tunnels > Authtokens, and add a token.

### Conclusion
Here you are, now you can access the power of LLMs without draining your bank account, frying your laptop or selling your soul to GPU providers.

You can directly download my notebook to upload it on Google Colab, 
